{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF+ NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-fIvB_l8-3",
        "outputId": "e39ed8d8-34a4-419a-d648-f1bfb67dd6bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaMulticore\n",
        "from pprint import pprint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98i6HorknMrP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeNubu4rJJHM"
      },
      "source": [
        "# Basicly Get Same Data and Processed Like Roger and Crystal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQGwfAc-mxjh"
      },
      "source": [
        "## Google News"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTcOOYsdmx1k"
      },
      "source": [
        "keywords_all = pd.read_csv('Keywords - Manual Group.csv')\n",
        "keywords_all.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGTjNqYQne6U"
      },
      "source": [
        "keywords = keywords_all.loc[keywords_all['Category'].isin(['Rule', 'Government', 'violation']), ['Category', 'Keywords']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scwUFwYdoO4Y"
      },
      "source": [
        "### Read the News Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr9AbYd_nfAP"
      },
      "source": [
        "# Not compliance articles\n",
        "google_news_finance = pd.read_excel('google_news_finance_1021.xlsx')\n",
        "google_news_finance['text'] = google_news_finance['title'] + '. ' + google_news_finance['summary']\n",
        "\n",
        "# Compliance articles\n",
        "google_news = pd.read_csv('google_news_1021.csv')\n",
        "google_news['text'] = google_news['title'] + '. ' + google_news['summary']\n",
        "\n",
        "df = pd.read_excel('NewsPaper_df.xlsx')\n",
        "df['text'] = df['Title'] + '. ' + df['Summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMohTeDenfFr",
        "outputId": "832883c8-fb41-4a0e-da0c-c68b5e4a117e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "google_news_finance.keyword.unique()\n",
        "df.Source.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['SEC', 'WSJ', 'The FCPA Blog', 'Corporate Compliance Insights',\n",
              "       'Compliance and Enforcement', 'Financial Services Perspectives'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axgv-cfzoUZE"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQUIrZiEnfJA"
      },
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def normalize_corpus(paper):\n",
        "  paper = paper.lower()\n",
        "  paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
        "  paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
        "  paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
        "  paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
        "  paper_tokens = list(filter(None, paper_tokens))\n",
        "  return paper_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js1grxeenfLx"
      },
      "source": [
        "compliance_news = pd.concat([google_news['text'], df['text']], ignore_index=True).dropna()\n",
        "non_compliance_news = google_news_finance['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9z14tCx5KZN"
      },
      "source": [
        "# Simple TF-IDF on Compliance news without Identify Compliance vs non-Compliance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCaoQhRrpJb1"
      },
      "source": [
        "### Creating Vocabulary and Word Counts to conduct TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yErcV3RdfmnA"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4K7YtrQkXq8"
      },
      "source": [
        "We applied sklearn tokenizer to tokenize and lemmarize. After unified the word format, we applied TF-IDF vectorizer to project text to word vectors, so the whole text is converted into a data frame with each content word as a feature and TF-IDF score as the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnasMdOqPlQG"
      },
      "source": [
        "# Applying TFIDF\n",
        "tfidf_vectors = TfidfVectorizer(tokenizer = lambda i:i,ngram_range = (1,3),\n",
        "                                preprocessor=normalize_corpus,\n",
        "                                max_df=0.85, lowercase=False) \n",
        "tfidf = tfidf_vectors.fit_transform(compliance_news)\n",
        "features = tfidf_vectors.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a82tE_dnnfg9"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1OT_cCznflX"
      },
      "source": [
        "sorted_items=sort_coo(tfidf.tocoo())\n",
        "keywords=extract_topn_from_vector(features,sorted_items,50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9Hw_wmJd5D"
      },
      "source": [
        "### Sort the keyword and pull result out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyc3EOjvnfjf",
        "outputId": "8a0b83aa-7329-4105-90e3-89b3ca3a796d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "sorted_keywords = sorted(keywords.items(), key=lambda kv: kv[1])\n",
        "import collections\n",
        "collections.OrderedDict(sorted_keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('bankruptcy', 0.392),\n",
              "             ('udf', 0.393),\n",
              "             ('chicken', 0.395),\n",
              "             ('fund', 0.396),\n",
              "             ('voter', 0.396),\n",
              "             ('disparate impact', 0.397),\n",
              "             ('disparate', 0.397),\n",
              "             ('tender', 0.398),\n",
              "             ('canada', 0.405),\n",
              "             ('mcafee', 0.408),\n",
              "             ('compliance date', 0.409),\n",
              "             ('ransomware', 0.409),\n",
              "             ('fair value', 0.41),\n",
              "             ('retail sale', 0.41),\n",
              "             ('delaware', 0.414),\n",
              "             ('fair canada', 0.414),\n",
              "             ('tier', 0.415),\n",
              "             ('circuit breaker', 0.416),\n",
              "             ('breaker', 0.416),\n",
              "             ('ethereum', 0.417),\n",
              "             ('commerzbank', 0.42),\n",
              "             ('etf', 0.423),\n",
              "             ('sbsds', 0.424),\n",
              "             ('vendor', 0.426),\n",
              "             ('campaign', 0.426),\n",
              "             ('rating', 0.428),\n",
              "             ('rbi', 0.429),\n",
              "             ('13f', 0.434),\n",
              "             ('midstream', 0.437),\n",
              "             ('madden', 0.441),\n",
              "             ('walmart', 0.444),\n",
              "             ('sandbox', 0.445),\n",
              "             ('truth', 0.446),\n",
              "             ('fortune', 0.449),\n",
              "             ('cowen', 0.451),\n",
              "             ('business plan', 0.459),\n",
              "             ('auction', 0.468),\n",
              "             ('flood insurance', 0.47),\n",
              "             ('flood', 0.47),\n",
              "             ('andeavor', 0.472),\n",
              "             ('esg', 0.475),\n",
              "             ('rias', 0.508),\n",
              "             ('5g', 0.527),\n",
              "             ('guggenheim', 0.537)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXF2b4DDyE27"
      },
      "source": [
        "# Name Entity, The Stanford NER jar file you can download here: https://stanfordnlp.github.io/CoreNLP/index.html#download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UokW_-i7J13M"
      },
      "source": [
        "The NER includes the names of people and organizations, organization address and zip code. Entity recognition is to identify entities with specific meaning in text, which includes Entity class(person name, place name, organization name),Time class(date) and Digital class(phone number, zip code)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWWX2SX4y_fb"
      },
      "source": [
        "#!  pip install polyglo\n",
        "#! pip install PyICU\n",
        "#! pip install pycld2\n",
        "#! pip install morfessor\n",
        "! polyglot download embeddings2.en\n",
        "! polyglot download ner2.en\n",
        "from polyglot.text import Text\n",
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "import re\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRuKDimIAR0_"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjyruN_ARJ8"
      },
      "source": [
        "def preprocess(doc):\n",
        "    # doc = doc.lower()\n",
        "    bad_symbols = [\"\\'s\", '.com', ':', \"\\'\", '\\\"', '(', ')', '-']\n",
        "    for removable in bad_symbols:\n",
        "        doc = doc.replace(removable, '')\n",
        "    return doc\n",
        "\n",
        "def filter_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Em4ZZKKHOl"
      },
      "source": [
        "There are three methods of entity recognition:<br/>\n",
        "1.   Linguistic grammar-based techniques : The Linguistic grammar-based techniques mainly based on grammar, and its application in the engineering implementation is to write a lot of regex, which can solve the recognition of time class and digital class named entities.\n",
        "2.   Statistical models: At present, the statistical methods are mainly HMM and CRF models, which are also relatively mature at present.\n",
        "\n",
        "3.   Deep learning models: The method of deep learning is the most popular way at present, especially the DL model of RNN series, which can absorb more text semantic information, and its effect is the best at present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSl0LD3hnfdY"
      },
      "source": [
        "def stanford_parse(text):\n",
        "    parent_dir = os.path.abspath('')\n",
        "    st = StanfordNERTagger(parent_dir + '/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "                           parent_dir + '/stanford-ner/stanford-ner-3.9.2.jar', encoding='utf-8')\n",
        "    tokenized_text = word_tokenize(text)\n",
        "    classified_text = st.tag(tokenized_text)\n",
        "    obj = ''\n",
        "    prev_tag = 'O'\n",
        "\n",
        "    ner_dict = {'PERSON': set(), 'LOCATION': set(), 'ORGANIZATION': set()}\n",
        "    for idx, (ne, net) in enumerate(classified_text):\n",
        "        # print(f\"ne is: {ne}\")\n",
        "        # print(f\"net is: {net}\")\n",
        "        if net != prev_tag and prev_tag != 'O':\n",
        "            if obj != '':\n",
        "                ner_dict[prev_tag].add(filter_numbers(obj.lower()))\n",
        "\n",
        "            obj = ''\n",
        "        elif net == prev_tag and net != 'O':\n",
        "            obj = obj + ' ' + ne\n",
        "\n",
        "        elif net != 'O' and prev_tag == 'O':\n",
        "            obj = ne\n",
        "\n",
        "            if idx == len(classified_text) - 1:\n",
        "                ner_dict[net].add(filter_numbers(obj.lower()))\n",
        "                break\n",
        "\n",
        "        prev_tag = net\n",
        "\n",
        "    # print(f\"ner_dict is: {ner_dict}\")\n",
        "    return ner_dict\n",
        "\n",
        "\n",
        "def polyglot_parse(text):\n",
        "    ents = Text(text).entities\n",
        "    # print(f\"Entities are: {ents}\")\n",
        "    m = {'I-PER': 'PERSON', 'I-LOC': 'LOCATION', 'I-ORG': 'ORGANIZATION'}\n",
        "    ner_dict = {'PERSON': set(), 'LOCATION': set(), 'ORGANIZATION': set()}\n",
        "\n",
        "    # print(f\"raw ner_dict is: {ner_dict}\")\n",
        "\n",
        "    for e in ents:\n",
        "        entity = ' '.join(e)\n",
        "        ner_dict[m[e.tag]].add(filter_numbers(entity.lower()))\n",
        "\n",
        "    # print(f\"result ner_dict is: {ner_dict}\")\n",
        "    return ner_dict\n",
        "\n",
        "\n",
        "def spacy_parse(text):\n",
        "    nlp = spacy.load('en')\n",
        "    sp = nlp(text)\n",
        "    spacy_entities = []\n",
        "    m = {'PERSON': 'PERSON', 'ORG': 'ORGANIZATION', 'GPE': 'LOCATION'}\n",
        "    ner_dict = {'PERSON': set(), 'ORGANIZATION': set(), 'LOCATION': set()}\n",
        "    for ent in sp.ents:\n",
        "        if ent.label_ in m:\n",
        "            ner_dict[m[ent.label_]].add(filter_numbers(ent.text.lower()))\n",
        "\n",
        "    # print(f\"result ner_dict is: {ner_dict}\")\n",
        "    return ner_dict\n",
        "\n",
        "\n",
        "def symm_agree(w1, w2):\n",
        "    return w1 in w2 or w2 in w1\n",
        "\n",
        "\n",
        "def get_smallest(w1, w2, w3):\n",
        "    if w1 in w2 and w1 in w3:\n",
        "        return w1\n",
        "    elif w2 in w1 and w2 in w3:\n",
        "        return w2\n",
        "    else:\n",
        "        return w3\n",
        "\n",
        "\n",
        "# Never selects the third ranked\n",
        "def get_biased_smallest(w1, w2, w3):\n",
        "    if w1 in w2 and w1 in w3:\n",
        "        return w1\n",
        "\n",
        "    elif w2 in w1 and w1 in w3:\n",
        "        return w1\n",
        "\n",
        "    elif w2 in w1 and w2 in w3:\n",
        "        return w2\n",
        "    else:\n",
        "        if w1 in w2:\n",
        "            return w1\n",
        "        else:\n",
        "            return w2\n",
        "\n",
        "def get_named_entities(Doc):\n",
        "    doc = preprocess(Doc)\n",
        "    try:\n",
        "        a = polyglot_parse(doc)\n",
        "        b = stanford_parse(doc)\n",
        "        c = spacy_parse(doc)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "    persons = vote(a, b, c, 'PERSON')\n",
        "    new_person = set()\n",
        "    bad_person = set()\n",
        "    for p in persons:\n",
        "        if len(p.split(' ')) == 4:\n",
        "            new_person.update([' '.join(p.split(' ')[:2]), ' '.join(p.split(' ')[2:])])\n",
        "            bad_person.add(p)\n",
        "        if len(p.split(' ')) == 1:\n",
        "            bad_person.add(p)\n",
        "\n",
        "    for p in bad_person:\n",
        "        persons.remove(p)\n",
        "    for p in new_person:\n",
        "        persons.add(p)\n",
        "\n",
        "    locations = vote(a, b, c, 'LOCATION')\n",
        "    orgs = vote(a, b, c, 'ORGANIZATION')\n",
        "\n",
        "    orgs = set([n for n in orgs if len(n.split(' ')) < 5])\n",
        "\n",
        "    return list(map(lambda p: p.title(), persons)), list(map(lambda l: l.title(), locations)), list(map(lambda o: o.title(), orgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujLpZTVhK9Ye"
      },
      "source": [
        "Here we adopted three NER models: **Stanford NER, polyglot and spacy entities.** <br/>\n",
        "StanfordNER is a java implementation of NER (named entity recognizer), it can mark the sequence of words in the text, such as person name, company name, gene name or protein name. It comes with a well-designed feature extractor for NER and many options for defining the feature extractor. There are many good English named entity recognizers, especially for person name, organization name, place name(Locations).<br/>\n",
        "Polyglot language detection relies on pycld2 and cld2, among which cld2 is a multilingual detection application developed by Google. The training corpus of polyglot entity recognition comes from Wikipedia (wiki). The trained model has not been installed for the first time, so it needs to download the corresponding model. Polyglot supports the identification of entity classes (person name, place name, organization name) in 40 languages.<br/>\n",
        "Spacy includes a fast entity recognition model – “spacy entities”, which can recognize entity phrases in documents. There are many types of entities, such as\n",
        "people, places, organizations, dates, numbers. You can access these entities through the ents property of the document.<br/>\n",
        "**We created weighted function vote() that can pick the most appropriate NER result from this three algrothiums. After recognized the Person’s Name, sometimes there’s more than 1 person’s name compacted together in the “Person Name” dictionary. Then we split different person’s name by the name length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0TYq9lZAGxa"
      },
      "source": [
        "def vote(poly, stanf, spac, ner_type):\n",
        "    ner = set()\n",
        "\n",
        "    # If poly is a subset of the others, use that\n",
        "\n",
        "    for poly_p in poly[ner_type]:\n",
        "        for stanf_p in stanf[ner_type]:\n",
        "            for spac_p in spac[ner_type]:\n",
        "                pst = symm_agree(poly_p, stanf_p)\n",
        "                ss = symm_agree(stanf_p, spac_p)\n",
        "                psp = symm_agree(poly_p, spac_p)\n",
        "\n",
        "                if pst and ss and psp:\n",
        "                    ner.add(get_biased_smallest(poly_p, stanf_p, spac_p))\n",
        "                elif pst or ss or psp:\n",
        "                    # Select what the actual named entity is.\n",
        "\n",
        "                    if psp:\n",
        "                        ner.add(poly_p)\n",
        "\n",
        "                    elif pst:\n",
        "                        if poly_p in stanf_p:\n",
        "                            ner.add(poly_p)\n",
        "                        else:\n",
        "                            ner.add(stanf_p)\n",
        "                    else:\n",
        "                        if stanf_p in spac_p:\n",
        "                            ner.add(stanf_p)\n",
        "                        else:\n",
        "                            ner.add(spac_p)\n",
        "\n",
        "    # At the end, take away all entities that are substrings of other entities\n",
        "\n",
        "    new_set = set()\n",
        "    blacklist = ['foundation', 'company', 'inc', 'corp', 'business', 'l.l.c', 'corporation', 'incorporated']\n",
        "    for ne1 in ner:\n",
        "        good = True\n",
        "        for ne2 in ner:\n",
        "            if ne1 != ne2 and ne1 in ne2:\n",
        "                good = False\n",
        "                break\n",
        "\n",
        "        if good and ne1 not in blacklist:\n",
        "            new_set.add(ne1)\n",
        "\n",
        "    return new_set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tittAs0LL9Kp"
      },
      "source": [
        "### Ignore Warnings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPax0VIBnfbP"
      },
      "source": [
        "import warnings\n",
        "warnings.warn(\"ignore\")\n",
        "People=[]\n",
        "Location=[]\n",
        "Orgnization=[]\n",
        "for i in range(len(df['text'])):\n",
        "  people,location,orgs = get_named_entities(df['text'][i])\n",
        "  People.append(people)\n",
        "  Location.append(location)\n",
        "  Orgnization.append(orgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTgVQnS-OdbX",
        "outputId": "15e67eaa-979e-448a-b8ce-1f2e96917cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NER = pd.DataFrame([People,Location,Orgnization])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Firm', 'Releasethe Securities', 'Securities And Exchange Commission'],\n",
              " ['Releasethe Securities', 'Exchange Commission', 'Finra'],\n",
              " ['U.S . Securities', 'Commodity Futures Trading Commission', 'Cftc'],\n",
              " ['Marathon', 'Sec'],\n",
              " ['Pricewaterhousecoopers',\n",
              "  'Pcaob Board',\n",
              "  'Releasethe Securities',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Technology'],\n",
              " ['Pcaob Board',\n",
              "  'Releasethe Securities',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Public Company'],\n",
              " ['Releasethe Securities', 'Exchange Commission'],\n",
              " ['Jbs', 'Sec', 'Pilgrim’S Pride Corporation', 'Ministerio Publico Federal'],\n",
              " ['Releasethe Securities',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Disclosure Review Program',\n",
              "  'Division Of Corporation Finance'],\n",
              " ['Division',\n",
              "  'Disclosure Review Program',\n",
              "  'Office Of Municipal Securities',\n",
              "  'Nrsros',\n",
              "  'Releasethe Securities',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Office Of Credit Ratings'],\n",
              " ['Sae', 'Sec'],\n",
              " ['Timmons',\n",
              "  'Releasethe Securities',\n",
              "  'New',\n",
              "  'Securities And Exchange Commission'],\n",
              " ['Releasethe Securities',\n",
              "  'Division Of Investment Management',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Investment Companies'],\n",
              " ['Sec'],\n",
              " ['Sec'],\n",
              " ['Mcafee', 'Sec', 'Department Of Justice'],\n",
              " ['Sec'],\n",
              " ['Sec'],\n",
              " ['Releasethe Securities',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Fimsac',\n",
              "  'Municipal Securities'],\n",
              " ['Releasethe Securities',\n",
              "  'Hilton Worldwide Holdings Inc.',\n",
              "  'Securities And Exchange Commission',\n",
              "  'Sec Charges Hospitality Company'],\n",
              " ['Releasethe Securities',\n",
              "  'Sec Charges Hp Inc.',\n",
              "  'Securities And Exchange Commission'],\n",
              " ['Sec'],\n",
              " ['Releasethe Securities', 'Exchange Commission'],\n",
              " ['Morgan Stanley & Co.',\n",
              "  'Releasethe Securities',\n",
              "  'Exchange Commission',\n",
              "  'Morgan Stanley Agrees'],\n",
              " ['Sec Charges Manitex International'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['Management'],\n",
              " [],\n",
              " ['Tcja'],\n",
              " [],\n",
              " [],\n",
              " ['American Airlines', 'Turbocharges', 'Technology'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['Walmart Leverages Data', 'Project Gigaton', 'Mclaughlin'],\n",
              " [],\n",
              " ['Federal Reserve'],\n",
              " ['Anticorruption'],\n",
              " ['Chávezera'],\n",
              " ['Fcpa', 'Doj', 'Sec', 'Investimentos S.A'],\n",
              " ['Citibank Mustcreate'],\n",
              " ['Sec', 'Fcpa', 'Jbs S.A.', 'Banco Nacional De', 'Doj'],\n",
              " ['Coalition', 'Integrity', 'Congress'],\n",
              " ['U.S. Department Of Justice', 'Ofac'],\n",
              " ['Wef', 'International Business Council'],\n",
              " ['European Union',\n",
              "  'Saudi Telecom Company',\n",
              "  'Integrity & Compliance Taskforce',\n",
              "  'Mastercard'],\n",
              " [],\n",
              " ['Kptl',\n",
              "  'Interamerican Development Bank',\n",
              "  'Kalpataru Power Transmission Ltd.',\n",
              "  'African Development Bank',\n",
              "  'Asian Development Bank',\n",
              "  'Grästorp Ab',\n",
              "  'Linjemontage',\n",
              "  'World Bank'],\n",
              " [],\n",
              " [],\n",
              " ['Governancelocation Lima',\n",
              "  'Jobs Basel Institute',\n",
              "  'Americaemployer Basel Institute'],\n",
              " ['Esg'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['Aml', 'Complyadvantage', 'Aca Compliance Groupaca', 'Aca’S'],\n",
              " ['Council', 'Gartner Risk Management'],\n",
              " ['Osha', 'Cal', 'Health'],\n",
              " ['Doj'],\n",
              " ['Hilton', 'Sec'],\n",
              " ['Congress', 'Ftc'],\n",
              " ['Mfa', 'Dfs', 'Twitter'],\n",
              " ['Deutsche Bank', 'Buzzfeed'],\n",
              " ['Equilar'],\n",
              " [],\n",
              " ['Supreme Court', 'Sec', 'Director Of Enforcement'],\n",
              " ['Sec', 'Bmw Ag', 'Bmw Na'],\n",
              " ['Sec'],\n",
              " [],\n",
              " [],\n",
              " ['Foreign', 'Treasury Department’S', 'Ofac'],\n",
              " ['Congress'],\n",
              " [],\n",
              " ['Fha'],\n",
              " ['National Flood Insurance',\n",
              "  'Farm Credit Administration',\n",
              "  'Federal Deposit Insurance Corporation',\n",
              "  'The Federal Reserve System',\n",
              "  'Biggertwaters Flood Insurance',\n",
              "  'National Credit Union Administration',\n",
              "  'Governors',\n",
              "  'Agencies'],\n",
              " ['Cdc', 'Public Health'],\n",
              " ['Navient Solutions', 'Fifth Circuit', 'Tenth Circuit Agrees'],\n",
              " ['Preparedness'],\n",
              " ['Circuit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    }
  ]
}
